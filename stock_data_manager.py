"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏ –∞–∫—Ü–∏–π —Å –ú–æ—Å–±–∏—Ä–∂–∏.
"""

import os
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import requests
import pandas as pd
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('stock_data_manager.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class StockDataManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∞–∫—Ü–∏–π –ú–æ—Å–±–∏—Ä–∂–∏."""

    BASE_URL = "https://iss.moex.com/iss/history/engines/stock/markets/shares/securities"
    DATA_DIR = Path("stock_data")
    BATCH_SIZE = 100  # –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø–∏—Å–µ–π –∑–∞ –∑–∞–ø—Ä–æ—Å

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞."""
        self._create_data_directory()
        self._setup_session()
        logger.info("StockDataManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _create_data_directory(self) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö."""
        self.DATA_DIR.mkdir(exist_ok=True)
        logger.info(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö: {self.DATA_DIR}")

    def _setup_session(self) -> None:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ —Å–±–æ–µ."""
        self.session = requests.Session()
        
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def _get_csv_path(self, ticker: str) -> Path:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Ç–∏–∫–µ—Ä–∞."""
        return self.DATA_DIR / f"{ticker}_full.csv"

    def _get_last_date_in_file(self, ticker: str) -> Optional[datetime]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –≤ —Ñ–∞–π–ª–µ CSV."""
        csv_path = self._get_csv_path(ticker)
        
        if not csv_path.exists():
            logger.info(f"–§–∞–π–ª –¥–ª—è {ticker} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–∞—á–Ω–µ–º —Å –Ω–∞—á–∞–ª–∞.")
            return None
        
        try:
            df = pd.read_csv(csv_path, parse_dates=['DATE'])
            if df.empty:
                logger.warning(f"–§–∞–π–ª {ticker} –ø—É—Å—Ç.")
                return None
            
            last_date = df['DATE'].max()
            logger.info(f"–ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –¥–ª—è {ticker}: {last_date.date()}")
            return last_date
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {ticker}: {e}")
            return None

    def _fetch_data_batch(
        self, 
        ticker: str, 
        start: int = 0
    ) -> Tuple[List[Dict], bool]:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö —Å API –ú–æ—Å–±–∏—Ä–∂–∏.
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            start: –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Å–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö, –µ—Å—Ç—å –ª–∏ –µ—â–µ –¥–∞–Ω–Ω—ã–µ)
        """
        url = f"{self.BASE_URL}/{ticker}.json"
        
        params = {
            'start': start,
            'limit': self.BATCH_SIZE
        }
        
        try:
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–≤–µ—Ç–µ
            if 'history' not in data or not data['history']['data']:
                logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker} —Å –ø–æ–∑–∏—Ü–∏–∏ {start}")
                return [], False
            
            history_data = data['history']['data']
            columns = data['history']['columns']
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            records = []
            for row in history_data:
                record = dict(zip(columns, row))
                records.append(record)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ –¥–∞–Ω–Ω—ã–µ
            has_more = len(history_data) == self.BATCH_SIZE
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(records)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {ticker} " 
                       f"(–ø–æ–∑–∏—Ü–∏—è {start}, –µ—â–µ: {has_more})")
            
            return records, has_more
        
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ API –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {ticker}: {e}")
            return [], False
        except (KeyError, json.JSONDecodeError) as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}: {e}")
            return [], False

    def download_stock_data(
        self, 
        ticker: str, 
        from_date: Optional[str] = None,
        to_date: Optional[str] = None
    ) -> pd.DataFrame:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –∞–∫—Ü–∏–∏ —Å –ú–æ—Å–±–∏—Ä–∂–∏.
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            from_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
            to_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
            
        Returns:
            DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}")
        
        all_records = []
        start = 0
        
        while True:
            records, has_more = self._fetch_data_batch(ticker, start)
            
            if not records:
                break
            
            all_records.extend(records)
            
            if not has_more:
                break
            
            start += self.BATCH_SIZE
        
        if not all_records:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker}")
            return pd.DataFrame()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
        df = pd.DataFrame(all_records)
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã (API –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å —Ä–∞–∑–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è)
        column_mapping = {
            'TRADEDATE': 'DATE',
            'OPEN': 'OPEN',
            'HIGH': 'HIGH',
            'LOW': 'LOW',
            'CLOSE': 'CLOSE',
            'VOLUME': 'VOLUME'
        }
        
        df = df.rename(columns=column_mapping)
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
        required_columns = ['DATE', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME']
        available_columns = [col for col in required_columns if col in df.columns]
        df = df[available_columns]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
        df['DATE'] = pd.to_datetime(df['DATE'])
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—ä–µ–º–æ–º (–¥–Ω–∏ –±–µ–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏)
        df = df[df['VOLUME'] > 0]
        logger.info(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥–Ω–µ–π –±–µ–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if from_date:
            from_dt = pd.to_datetime(from_date)
            before_filter = len(df)
            df = df[df['DATE'] >= from_dt]
            logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ >= {from_date}: {len(df)} –∑–∞–ø–∏—Å–µ–π (–±—ã–ª–æ {before_filter})")
        
        if to_date:
            to_dt = pd.to_datetime(to_date)
            before_filter = len(df)
            df = df[df['DATE'] <= to_dt]
            logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ <= {to_date}: {len(df)} –∑–∞–ø–∏—Å–µ–π (–±—ã–ª–æ {before_filter})")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
        df = df.sort_values('DATE').reset_index(drop=True)
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {ticker}")
        
        return df

    def _merge_data(
        self, 
        existing_df: pd.DataFrame, 
        new_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.
        
        Args:
            existing_df: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            new_df: –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame
        """
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±–∞ DataFrame
        merged_df = pd.concat([existing_df, new_df], ignore_index=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è—è –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é
        merged_df = merged_df.drop_duplicates(subset=['DATE'], keep='last')
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
        merged_df = merged_df.sort_values('DATE').reset_index(drop=True)
        
        return merged_df

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –û—á–∏—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ: —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏ –¥–∞—Ç –∏ –¥–Ω–∏ –±–µ–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏.
        
        Args:
            df: DataFrame –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π DataFrame
        """
        try:
            # –®–∞–≥ 1: –£–¥–∞–ª—è–µ–º –¥–Ω–∏ –±–µ–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏ (VOLUME=0)
            before_volume = len(df)
            df = df[df['VOLUME'] > 0]
            volume_removed = before_volume - len(df)
            if volume_removed > 0:
                logger.info(f"  üßπ –£–¥–∞–ª–µ–Ω—ã –¥–Ω–∏ –±–µ–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏: {volume_removed} —Å—Ç—Ä–æ–∫")
            
            # –®–∞–≥ 2: –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –¥–∞—Ç (–¥–≤–µ —Å–µ—Å—Å–∏–∏ —Ç–æ—Ä–≥–æ–≤–ª–∏ –†–ü–° + T+0)
            # –ë–µ—Ä–µ–º —Å–µ—Å—Å–∏—é —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º (–æ—Å–Ω–æ–≤–Ω–∞—è T+0)
            if df.duplicated(subset=['DATE']).any():
                before_dups = len(df)
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏ –æ–±—ä–µ–º—É (—É–±—ã–≤–∞–Ω–∏–µ), –∑–∞—Ç–µ–º –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é (max volume)
                df = df.sort_values(['DATE', 'VOLUME'], ascending=[True, False])
                df = df.drop_duplicates(subset=['DATE'], keep='first')
                dups_removed = before_dups - len(df)
                logger.info(f"  ‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã –¥–≤–æ–π–Ω—ã–µ —Å–µ—Å—Å–∏–∏: {dups_removed} —É–¥–∞–ª–µ–Ω–æ")
            
            # –®–∞–≥ 3: –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
            df = df.sort_values('DATE').reset_index(drop=True)
            
            return df
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return df

    def save_to_csv(self, ticker: str, data: pd.DataFrame) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ CSV —Ñ–∞–π–ª.
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            data: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        try:
            csv_path = self._get_csv_path(ticker)
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ DATE –≤ —Ñ–æ—Ä–º–∞—Ç–µ string –¥–ª—è CSV
            df_to_save = data.copy()
            df_to_save['DATE'] = df_to_save['DATE'].dt.strftime('%Y-%m-%d')
            
            df_to_save.to_csv(csv_path, index=False)
            logger.info(f"–î–∞–Ω–Ω—ã–µ {ticker} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
            return True
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {ticker}: {e}")
            return False

    def update_watchlist(self, tickers_list: List[str]) -> Dict[str, bool]:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ –∞–∫—Ü–∏–π.
        
        Args:
            tickers_list: –°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        """
        results = {}
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è {len(tickers_list)} –∞–∫—Ü–∏–π")
        
        for ticker in tickers_list:
            try:
                logger.info(f"\n--- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {ticker} ---")
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º —Ñ–∞–π–ª–µ
                last_date = self._get_last_date_in_file(ticker)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—É—é –¥–∞—Ç—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                if last_date:
                    # –ù–∞—á–∏–Ω–∞–µ–º —Å–æ –¥–Ω—è –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                    from_date = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')
                    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å {from_date}")
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–π–ª–∞, –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    # –ù–∞–ø—Ä–∏–º–µ—Ä, –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥
                    one_year_ago = datetime.now() - timedelta(days=365)
                    from_date = one_year_ago.strftime('%Y-%m-%d')
                    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å {from_date}")
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                new_data = self.download_stock_data(ticker, from_date=from_date)
                
                if new_data.empty:
                    logger.warning(f"–ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}")
                    results[ticker] = False
                    continue
                
                # –ï—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                if last_date is not None:
                    existing_data = pd.read_csv(
                        self._get_csv_path(ticker),
                        parse_dates=['DATE']
                    )
                    merged_data = self._merge_data(existing_data, new_data)
                    logger.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}: "
                               f"{len(existing_data)} + {len(new_data)} = {len(merged_data)}")
                else:
                    merged_data = new_data
                
                # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ (—É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –∏ –¥–Ω–∏ –±–µ–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏)
                logger.info(f"üîß –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö {ticker}:")
                merged_data = self._clean_data(merged_data)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                success = self.save_to_csv(ticker, merged_data)
                results[ticker] = success
                
                if success:
                    logger.info(f"‚úì {ticker} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω ({len(merged_data)} –∑–∞–ø–∏—Å–µ–π)")
                else:
                    logger.warning(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {ticker}")
            
            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {ticker}: {e}")
                results[ticker] = False
        
        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        successful = sum(1 for v in results.values() if v)
        logger.info(f"\n{'='*50}")
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {successful}/{len(tickers_list)} —É—Å–ø–µ—à–Ω–æ")
        logger.info(f"{'='*50}\n")
        
        return results

    def get_data(self, ticker: str) -> Optional[pd.DataFrame]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–∏–∫–µ—Ä—É.
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            
        Returns:
            DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None
        """
        csv_path = self._get_csv_path(ticker)
        
        if not csv_path.exists():
            logger.warning(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {ticker} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return None
        
        try:
            df = pd.read_csv(csv_path, parse_dates=['DATE'])
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            return df
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö {ticker}: {e}")
            return None

    def get_statistics(self, ticker: str) -> Dict:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–∞–Ω–Ω—ã–º –∞–∫—Ü–∏–∏.
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        df = self.get_data(ticker)
        
        if df is None or df.empty:
            return {}
        
        return {
            'ticker': ticker,
            'total_records': len(df),
            'date_from': df['DATE'].min().strftime('%Y-%m-%d'),
            'date_to': df['DATE'].max().strftime('%Y-%m-%d'),
            'avg_price': df['CLOSE'].mean(),
            'min_price': df['CLOSE'].min(),
            'max_price': df['CLOSE'].max(),
            'total_volume': df['VOLUME'].sum()
        }


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞."""
    manager = StockDataManager()
    
    # –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∞–∫—Ü–∏–π –ú–æ—Å–±–∏—Ä–∂–∏
    tickers = ['SBER', 'GAZP', 'NVTK', 'PHOR', 'TATN']
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    results = manager.update_watchlist(tickers)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n" + "="*60)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ê–ö–¶–ò–Ø–ú")
    print("="*60)
    
    for ticker in tickers:
        stats = manager.get_statistics(ticker)
        if stats:
            print(f"\n{ticker}:")
            print(f"  –ó–∞–ø–∏—Å–µ–π: {stats['total_records']}")
            print(f"  –ü–µ—Ä–∏–æ–¥: {stats['date_from']} - {stats['date_to']}")
            print(f"  –¶–µ–Ω–∞ (—Å—Ä–µ–¥–Ω—è—è): {stats['avg_price']:.2f}")
            print(f"  –¶–µ–Ω–∞ (min-max): {stats['min_price']:.2f} - {stats['max_price']:.2f}")
            print(f"  –û–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤: {stats['total_volume']:,.0f}")


if __name__ == "__main__":
    main()

